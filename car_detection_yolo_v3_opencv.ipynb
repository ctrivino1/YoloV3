{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ctrivino1/YoloV3/blob/main/car_detection_yolo_v3_opencv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/ctrivino1/YoloV3.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpIc8P31yFtd",
        "outputId": "4f88d51c-211e-42a2-e743-58517eb18a97"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'YoloV3'...\n",
            "remote: Enumerating objects: 1192, done.\u001b[K\n",
            "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 1192 (delta 0), reused 9 (delta 0), pack-reused 1183\u001b[K\n",
            "Receiving objects: 100% (1192/1192), 111.88 MiB | 21.51 MiB/s, done.\n",
            "Updating files: 100% (1182/1182), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction**\n",
        "---\n",
        "\n",
        "**Object detection** refers to the **task of locating and classifying objects** within an **image or video**. In the context of **car object detection**, the problem involves **identifying and localizing all instances of cars within an image or video frame**.\n",
        "\n",
        "The problem of **car object detection** can be broken down into **two main sub-tasks**:\n",
        "\n",
        "* **Object localization**: This involves identifying the **location** of **all cars** within the **image or video frame**. Typically, **object localization** is done by **drawing bounding boxes** around the **objects of interest**.\n",
        "\n",
        "* **Object classification**: Once the objects have been localized, the next step is to **classify them into different categories**. In the case of **car object detection**, this **involves distinguishing between cars and other objects in the scene**, such as **pedestrians, buildings, and trees**.\n",
        "\n",
        "Both of these **sub-tasks can be challenging**, especially in **real-world scenarios** where the **images or videos may contain cluttered backgrounds**, **occlusions**, and **variations in lighting** and **camera angles**. To address **these challenges**, various **computer vision techniques** have been developed, including **deep learning-based approaches** such as **YOLO, Faster R-CNN, and Mask R-CNN**, which are capable of **achieving high levels of accuracy and efficiency in car object detection tasks**.\n",
        "\n",
        "**Data Structure**\n",
        "---\n",
        "\n",
        "The data provided consists of **two subdirectories**, **one for training images** and the other for **testing images**. The **training images directory** contains **1001 images** that will be used to **train the model**, while the **testing images directory** contains **175 images** that will be used to **evaluate the model**.\n",
        "\n",
        "In addition to **these subdirectories**, the **data also includes two CSV files**. One of these CSV files contains the **target labels assigned to each image in the training set**, which will be used to **train the model**. The **other CSV** file is **a sample submission file**, which is provided to the **participants** for **submitting their testing results.**\n",
        "\n",
        "**Notebook Structure**\n",
        "---\n",
        "\n",
        "This notebook is divided into four main sections, each with a specific focus:\n",
        "\n",
        "* **SetUp**: This section is dedicated to importing all the necessary modules required for the notebook to function. Additionally, we set some constants, which are used throughout the notebook for data loading.\n",
        "\n",
        "* **Data Loading**: In this section, we load the data in the form of Numpy arrays. The data contains images and bounding boxes. Although the bounding boxes are not required, they are still loaded in case they are needed in the future.\n",
        "\n",
        "* **Data Visualization**: Here, we create a plot that allows us to visualize the images present in the data along with their respective bounding boxes. This is an essential step in understanding the data and verifying that the data has been loaded correctly.\n",
        "\n",
        "* **YOLO**: In this section, we focus on the model - YOLOv3. We develop multiple functions that are used to make predictions on the data. These functions include filtering the predictions based on probability, applying non-max suppression, and finally, creating a function that brings everything together. The final function combines all the previously developed functions and provides the predicted bounding boxes, class probabilities, and labels for each image in the data. This section is the crux of the notebook and helps us to understand how YOLOv3 works in object detection."
      ],
      "metadata": {
        "id": "4Z1czUQOyEW0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SetUp**\n",
        "---"
      ],
      "metadata": {
        "id": "QxeQipQuyEXG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before proceeding with the **data loading, preprocessing, and model building steps**, it is important to **define the hyperparameters and constants** that will be used **throughout the notebook**. This will help in keeping the **code organized** and making it easier to tweak different parameters and observe the results."
      ],
      "metadata": {
        "id": "SgmDC1sKyEXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Common\n",
        "import os\n",
        "import random\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import clear_output as cls\n",
        "\n",
        "# Data\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm \n",
        "import tensorflow.data as tfd\n",
        "\n",
        "# Data Visualization\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-10T14:42:24.73832Z",
          "iopub.execute_input": "2023-03-10T14:42:24.738805Z",
          "iopub.status.idle": "2023-03-10T14:42:24.746385Z",
          "shell.execute_reply.started": "2023-03-10T14:42:24.738765Z",
          "shell.execute_reply": "2023-03-10T14:42:24.744911Z"
        },
        "trusted": true,
        "id": "xLN9dtHOyEXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparams\n",
        "IMAGE_HEIGHT = 380\n",
        "IMAGE_WIDTH = 676\n",
        "IMAGE_SIZE = (IMAGE_HEIGHT, IMAGE_WIDTH, 3)\n",
        "\n",
        "# Constants\n",
        "TRAIN_ROOT_DIR = '/content/YoloV3/data/training_images'\n",
        "TEST_ROOT_DIR = '/content/YoloV3/data/testing_images'\n",
        "TRAIN_CSV_PATH = '/content/YoloV3/data/train_solution_bounding_boxes (1).csv'\n",
        "\n",
        "# Threshold values\n",
        "PROB_THRESH = 0.9\n",
        "IoU_THRESH = 0.5\n",
        "\n",
        "# Class names.\n",
        "f = open('/content/YoloV3/yolo-coco-data/coco.names', 'rb')\n",
        "labels = list(n.decode('UTF-8').replace('\\n', ' ').strip() for n in f.readlines())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-10T14:42:24.761704Z",
          "iopub.execute_input": "2023-03-10T14:42:24.762225Z",
          "iopub.status.idle": "2023-03-10T14:42:24.772076Z",
          "shell.execute_reply.started": "2023-03-10T14:42:24.762176Z",
          "shell.execute_reply": "2023-03-10T14:42:24.770449Z"
        },
        "trusted": true,
        "id": "Ox6MQiityEXS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's set up a **random seed**, setting up a **random seed** is essential in a **machine learning project** for the following reasons:\n",
        "\n",
        "* **Reproducibility**: Setting up a random seed ensures that the results obtained from the model are reproducible. When a random seed is set, the same random numbers are generated each time the model is run. This means that the same results can be obtained every time the code is executed, which is critical for debugging, testing, and reproducing results.\n",
        "\n",
        "* **Consistency**: Setting up a random seed ensures that the same set of samples are selected for training and testing the model every time it is run. This consistency in the data selection process allows us to compare the performance of different models with the same data.\n",
        "\n",
        "* **Fairness**: In some cases, the random initialization of model parameters can lead to different results due to variations in the random seed. This can be particularly important in fairness-sensitive applications, such as when the model is used to make decisions that impact people's lives. By setting a random seed, we can reduce the impact of random variation on the model's performance and ensure that the model is fair to all groups."
      ],
      "metadata": {
        "id": "jV62mniYyEXU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Seed\n",
        "SEED = 42\n",
        "np.random.seed(SEED)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-10T14:42:24.792538Z",
          "iopub.execute_input": "2023-03-10T14:42:24.793069Z",
          "iopub.status.idle": "2023-03-10T14:42:24.799674Z",
          "shell.execute_reply.started": "2023-03-10T14:42:24.793021Z",
          "shell.execute_reply": "2023-03-10T14:42:24.798245Z"
        },
        "trusted": true,
        "id": "UmPDA2SVyEXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **number** **42** is **commonly used** as a **random seed** in the **machine learning community** for **historical and cultural reasons**. In the book ***The Hitchhiker's Guide to the Galaxy*** by *Douglas Adams*, the number **42** is famously referred to as the ***Answer to the Ultimate Question of Life, the Universe, and Everything.*** This has led to the **number 42** being adopted as a **sort of inside joke** or **cultural reference** in the **programming community**."
      ],
      "metadata": {
        "id": "wwSpeYTzyEXX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Loading**\n",
        "---\n",
        "\n",
        "Examining the **metadata** provided in the **dataset** can help in **easily locating** the **training images** and **their respective labels**. This information can be found in the **CSV file** that contains the **target labels** assigned to each image in the **training set**, and may include columns for the **image filename**, **class label**, and **additional information**."
      ],
      "metadata": {
        "id": "cEn3Yf_RyEXZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load metadata\n",
        "metadata = pd.read_csv(TRAIN_CSV_PATH)\n",
        "\n",
        "# Quick look\n",
        "metadata.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-10T14:42:24.826156Z",
          "iopub.execute_input": "2023-03-10T14:42:24.826673Z",
          "iopub.status.idle": "2023-03-10T14:42:24.84527Z",
          "shell.execute_reply.started": "2023-03-10T14:42:24.82662Z",
          "shell.execute_reply": "2023-03-10T14:42:24.84426Z"
        },
        "trusted": true,
        "id": "kMEZL812yEXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As expected, the CSV file that contains the **target labels** assigned to **each image** in the **training set** has the **image names** and their **respective bounding boxes**. The **bounding boxes** are **represented** using the **coordinates** of **the corners** of the **rectangle** that **encloses the object in the image**.\n",
        "\n",
        "In **object detection**, there are **two common ways** to represent **the bounding boxes**:\n",
        "\n",
        "* **Corners representation**: The bounding box is represented by the coordinates of its top-left and bottom-right corners. This is the method used in the provided CSV file.\n",
        "\n",
        "* **Center representation**: The bounding box is represented by the coordinates of its center point, along with its width and height. This is sometimes referred to as the \"midpoint and height/width\" representation.\n",
        "\n",
        "Both **representations are equivalent** and can be **converted to each other**, but they have **different advantages** depending on the **use case**. For example, the **corner representation** may be more **intuitive for humans to understand**, while the **center representation** may be **more convenient for calculating the intersection-over-union (IOU) metric** used to evaluate the accuracy of object detection models.\n",
        "\n",
        "---\n",
        "Here we create a **utility function** that can be used to **load images**. This function will be helpful in **loading images** when we start working with the **complete dataset**. Additionally, we will create a **load_dataset function** that will use this **utility function** to **load the entire dataset.**"
      ],
      "metadata": {
        "id": "GCKKI9fcyEXc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image(file_name: str, ROOT_PATH: str) -> tf.Tensor:\n",
        "    '''\n",
        "    Loads an image from the file path provided, performs necessary preprocessing steps, and returns the image as a tensor.\n",
        "\n",
        "    Args:\n",
        "        file_name (str): Name of the image file to load.\n",
        "        ROOT_PATH (str): Root directory where the image file is stored.\n",
        "\n",
        "    Returns:\n",
        "        image (tf.Tensor): Tensor representing the loaded image, normalized between 0 and 1, with data type float32.\n",
        "    '''\n",
        "    \n",
        "    # Obtain the complete path to the image file\n",
        "    image_path = os.path.join(ROOT_PATH, file_name)\n",
        "    \n",
        "    # Read the image file\n",
        "    image = tf.io.read_file(image_path)\n",
        "    \n",
        "    # Decode the image to tensor with 3 channels\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    \n",
        "    # Convert the image data type to float32\n",
        "    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
        "    \n",
        "    # Normalize the image to [0, 1] range\n",
        "    image = tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)\n",
        "    \n",
        "    return image\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-10T14:42:24.847138Z",
          "iopub.execute_input": "2023-03-10T14:42:24.847533Z",
          "iopub.status.idle": "2023-03-10T14:42:24.856857Z",
          "shell.execute_reply.started": "2023-03-10T14:42:24.847495Z",
          "shell.execute_reply": "2023-03-10T14:42:24.855327Z"
        },
        "trusted": true,
        "id": "PUwp61vjyEXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have loaded the **metadata** and created a **utility function** to load **individual images**, it's time to create a **more comprehensive function** that will **integrate over all the images listed in the metadata and load them for us**. This will enable us to perform **further data operations**, and eventually use the **loaded data** for **model training**."
      ],
      "metadata": {
        "id": "tAIQfsHjyEXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(\n",
        "    metadata: pd.DataFrame=metadata,\n",
        "    ROOT_PATH: str=TRAIN_ROOT_DIR,\n",
        "    SHUFFLE: bool=True,\n",
        "    SPLIT_RATIO: float=0.1) -> tuple:\n",
        "    \n",
        "    \"\"\"\n",
        "    Loads and returns a dataset of images and their corresponding bounding boxes based on the provided metadata file.\n",
        "\n",
        "    Args:\n",
        "        metadata (pd.DataFrame): A pandas DataFrame containing the metadata information for the dataset.\n",
        "        ROOT_PATH (str): The root path of the dataset.\n",
        "        SHUFFLE (bool, optional): Whether or not to shuffle the dataset. Defaults to True.\n",
        "        SPLIT_RATIO (float, optional): The ratio for splitting the dataset into training and validation sets. \n",
        "            If set to None, no split is performed. Defaults to 0.1.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If SPLIT_RATIO is not between 0 and 1.\n",
        "        FileNotFoundError: If the ROOT_PATH does not exist.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the dataset images and their corresponding bounding boxes as numpy arrays.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Check if SPLIT_RATIO is between 0 and 1.\n",
        "    if SPLIT_RATIO is not None and (SPLIT_RATIO < 0 or SPLIT_RATIO > 1):\n",
        "        raise ValueError(\"SPLIT_RATIO must be between 0 and 1.\")\n",
        "    \n",
        "    # Check if the ROOT_PATH is valid and exists.\n",
        "    if not os.path.exists(ROOT_PATH):\n",
        "        raise FileNotFoundError(f\"The ROOT_PATH {ROOT_PATH} does not exist.\")\n",
        "    \n",
        "    # Collect all the image paths.\n",
        "    image_paths = metadata['image']\n",
        "    \n",
        "    # Create space for storing the images and the bounding boxes\n",
        "    images = np.empty(shape=(len(image_paths), *IMAGE_SIZE), dtype=np.float32)\n",
        "    bounding_boxes = np.empty(shape=(len(image_paths), 4), dtype=np.float32)\n",
        "    \n",
        "    # Iterate over the image paths\n",
        "    index = 0\n",
        "    for image_path in tqdm(image_paths, desc=\"Loading\"):\n",
        "        try:\n",
        "            # Load the image\n",
        "            image = load_image(file_name = image_path, ROOT_PATH = ROOT_PATH)\n",
        "\n",
        "            # Extract the Corner Points\n",
        "            box = np.array([\n",
        "                metadata['xmin'][index],\n",
        "                metadata['ymin'][index],\n",
        "                metadata['xmax'][index],\n",
        "                metadata['ymax'][index],\n",
        "            ], dtype=np.float32)\n",
        "\n",
        "            # Add the data \n",
        "            images[index] = image\n",
        "            bounding_boxes[index] = box\n",
        "\n",
        "            # Increment the index\n",
        "            index  += 1\n",
        "        except:\n",
        "            # Raise an error if any of the image paths is invalid\n",
        "            raise IndexError(f\"Invalid image path: {image_path}\")\n",
        "    \n",
        "    # Return complete data\n",
        "    return images, bounding_boxes"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-10T14:42:24.867657Z",
          "iopub.execute_input": "2023-03-10T14:42:24.86816Z",
          "iopub.status.idle": "2023-03-10T14:42:24.882434Z",
          "shell.execute_reply.started": "2023-03-10T14:42:24.86811Z",
          "shell.execute_reply": "2023-03-10T14:42:24.880648Z"
        },
        "trusted": true,
        "id": "HZoHglUxyEXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's time to load the data."
      ],
      "metadata": {
        "id": "71kSAYJHyEXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and valid Dataset\n",
        "images, bounding_boxes = load_dataset()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-10T14:42:24.886828Z",
          "iopub.execute_input": "2023-03-10T14:42:24.887843Z",
          "iopub.status.idle": "2023-03-10T14:42:34.201101Z",
          "shell.execute_reply.started": "2023-03-10T14:42:24.887777Z",
          "shell.execute_reply": "2023-03-10T14:42:34.19974Z"
        },
        "trusted": true,
        "id": "IWlqPSZ5yEXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Visualization**"
      ],
      "metadata": {
        "id": "LOW1mMuryEXl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To better **understand and verify the bounding boxes** in **our dataset**, we will **create a function** that allows us to **visualize the data**. This **function will have the capability** of **adding a rectangle** over the image using **OpenCV library**. This will help us to see the **location and size of the bounding box** and ensure that it is **accurately capturing** the **object of interest** in the image. We can also use this **function to visualize the output of our model predictions on new images**. "
      ],
      "metadata": {
        "id": "Umx-JzaxyEXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_images_and_bbs(data: tfd.Dataset, GRID: list=[6,3], FIGSIZE: tuple=(30,40)) -> None:\n",
        "    \"\"\"\n",
        "    Visualizes a batch of images with their bounding boxes.\n",
        "    \n",
        "    Args:\n",
        "    - data: A TensorFlow dataset.\n",
        "    - GRID: A list specifying the number of rows and columns in the plot grid. Default is [6,3].\n",
        "    - FIGSIZE: A tuple specifying the size of the plot. Default is (30,40).\n",
        "    \n",
        "    Returns: None\n",
        "    \"\"\"\n",
        "    \n",
        "    # Define bounding box color\n",
        "    BB_COLOR = (0, 255, 0)\n",
        "    \n",
        "    # Plotting Configuration\n",
        "    plt.figure(figsize=FIGSIZE)\n",
        "    n_rows, n_cols = GRID\n",
        "    n_images = n_rows * n_cols\n",
        "    \n",
        "    # Gather the data\n",
        "    images, boxes = data[0], data[1]\n",
        "    \n",
        "    # Iterate over the data\n",
        "    for index, (image, box) in enumerate(zip(images, boxes)):\n",
        "        \n",
        "        # Convert the Bounding Box\n",
        "        x1, y1, x2, y2 = map(int, box)\n",
        "        \n",
        "        # Add the rectangle \n",
        "        image = cv.rectangle(\n",
        "            img = image, \n",
        "            pt1 = (x1, y1),\n",
        "            pt2 = (x2, y2),\n",
        "            thickness=5,\n",
        "            color=BB_COLOR\n",
        "        )\n",
        "        \n",
        "        # plot the image\n",
        "        plt.subplot(n_rows, n_cols, index+1)\n",
        "        plt.imshow(image)\n",
        "        plt.axis('off')\n",
        "        \n",
        "        # Break the loop\n",
        "        if (index+1)>=n_images:\n",
        "            break\n",
        "    \n",
        "    # Show final plot\n",
        "    plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-10T14:42:34.204235Z",
          "iopub.execute_input": "2023-03-10T14:42:34.204746Z",
          "iopub.status.idle": "2023-03-10T14:42:34.215454Z",
          "shell.execute_reply.started": "2023-03-10T14:42:34.204694Z",
          "shell.execute_reply": "2023-03-10T14:42:34.214398Z"
        },
        "trusted": true,
        "id": "NU8ynuHxyEXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Data\n",
        "dataset = [images, bounding_boxes]\n",
        "show_images_and_bbs(data = dataset)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-10T14:42:34.216923Z",
          "iopub.execute_input": "2023-03-10T14:42:34.217324Z",
          "iopub.status.idle": "2023-03-10T14:42:40.325233Z",
          "shell.execute_reply.started": "2023-03-10T14:42:34.217286Z",
          "shell.execute_reply": "2023-03-10T14:42:40.323978Z"
        },
        "trusted": true,
        "id": "EDK_Px_9yEXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Great progress!** With the ability to **visualize the images and draw bounding boxes around the objects**, we have a **better understanding** of the **data we're working with**. Now, we can focus on **building the model** that will **learn to detect** and **localize objects in these images**."
      ],
      "metadata": {
        "id": "LmysozR7yEXp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **YOLOv3 - You Only Look Once**\n",
        "---\n",
        "**YOLO (You Only Look Once)** is a **popular object detection algorithm** that can **recognize objects** in **images and videos in real-time**. **YOLOv3 is the latest version** of this **algorithm** and has **significantly improved the detection accuracy compared to its earlier versions**.\n",
        "\n",
        "The **main idea** behind **YOLO** is to **divide the input image** into a **grid of cells** and for each cell, the **algorithm predicts** a set of **bounding boxes**, **confidence scores**, and **class probabilities**. The **predicted bounding boxes** are parameterized relative to the **corresponding cell**, and they **predict the center coordinates**, **width, and height of the object**. The **confidence score** indicates how **confident the algorithm** is that there is an **object in that box**, and the **class probabilities predict** the **probability of the object belonging to each class**.\n",
        "\n",
        "**YOLOv3** introduces a **number of improvements** over its predecessor, including:\n",
        "\n",
        "* **Feature Pyramid Network (FPN)**: YOLOv3 uses an FPN to extract features at different scales, allowing it to detect objects of different sizes.\n",
        "\n",
        "* **Darknet-53 Backbone**: YOLOv3 uses a deep neural network architecture called Darknet-53 as its backbone. Darknet-53 is a powerful convolutional neural network that can extract features from images at different levels of abstraction.\n",
        "\n",
        "* **Spatially-Separable Convolutional Layers**: YOLOv3 uses a new type of convolutional layer called spatially-separable convolutional layers. These layers are more computationally efficient and allow YOLOv3 to run faster on GPUs.\n",
        "\n",
        "Overall, **YOLOv3 is a fast** and **accurate object detection algorithm** that can be used for a **wide range of applications, from surveillance to autonomous driving**.\n",
        "\n",
        "The **most easiest and convenient way** to use **Yolo is through OpenCV**."
      ],
      "metadata": {
        "id": "nM_nhz2HyEXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialized in Network\n",
        "net = cv.dnn.readNet(\n",
        "    '/kaggle/input/yolo-coco-data/yolov3.weights', \n",
        "    '/kaggle/input/yolo-coco-data/yolov3.cfg')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-10T14:42:40.32754Z",
          "iopub.execute_input": "2023-03-10T14:42:40.328193Z",
          "iopub.status.idle": "2023-03-10T14:42:43.971597Z",
          "shell.execute_reply.started": "2023-03-10T14:42:40.328149Z",
          "shell.execute_reply": "2023-03-10T14:42:43.970167Z"
        },
        "trusted": true,
        "id": "e8TdpsWwyEXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Names of the layer\n",
        "layer_names = net.getLayerNames()\n",
        "print(f\"Model Layer Name : \\n{layer_names}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-10T14:42:43.973416Z",
          "iopub.execute_input": "2023-03-10T14:42:43.973813Z",
          "iopub.status.idle": "2023-03-10T14:42:43.981054Z",
          "shell.execute_reply.started": "2023-03-10T14:42:43.973775Z",
          "shell.execute_reply": "2023-03-10T14:42:43.979556Z"
        },
        "trusted": true,
        "id": "Lw_0TSuoyEXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the output layers of the network\n",
        "output_layers_names = net.getUnconnectedOutLayersNames()\n",
        "output_layers_names"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-10T14:42:43.982777Z",
          "iopub.execute_input": "2023-03-10T14:42:43.983215Z",
          "iopub.status.idle": "2023-03-10T14:42:43.996928Z",
          "shell.execute_reply.started": "2023-03-10T14:42:43.983171Z",
          "shell.execute_reply": "2023-03-10T14:42:43.995468Z"
        },
        "trusted": true,
        "id": "n5bJ8pQ3yEXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OpenCV's** **deep neural network (dnn)** module expects the **input image** to be in a **specific format** called a **\"blob\"**. A **blob** is a **4D numpy array** **(batch_size, channels, height, width)** that contains the **input image** after **some pre-processing**. The blob can be fed directly into the **deep neural network** for **inference**. So let's create a function that will **convert the images into blobs**."
      ],
      "metadata": {
        "id": "wz7PVGj2yEXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def image_to_blob(image: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Convert an array of images into a 4D NumPy array in blob format. \n",
        "\n",
        "    Args:\n",
        "        images (np.ndarray): An array of images.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: A 4D NumPy array in blob format, where the first dimension\n",
        "        represents the number of images, the second and third dimensions represent\n",
        "        the size of the images, and the fourth dimension represents the color \n",
        "        channels.\n",
        "    \"\"\"\n",
        "\n",
        "    # Prepare a blob of image\n",
        "    blob = cv.dnn.blobFromImage(\n",
        "        image = image,\n",
        "        size = (416, 416), \n",
        "        mean = (0, 0, 0), \n",
        "        swapRB=True, \n",
        "        crop=False\n",
        "    )\n",
        "        \n",
        "    # Return blob\n",
        "    return blob"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-10T14:42:43.999033Z",
          "iopub.execute_input": "2023-03-10T14:42:44.000177Z",
          "iopub.status.idle": "2023-03-10T14:42:44.007794Z",
          "shell.execute_reply.started": "2023-03-10T14:42:44.000129Z",
          "shell.execute_reply": "2023-03-10T14:42:44.006448Z"
        },
        "trusted": true,
        "id": "yaMWBuPtyEXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A **blob** is a **multi-dimensional array** that contains the **image pixel values** and is processed by **deep learning models**.\n",
        "\n",
        "The `cv.dnn.blobFromImage()` function takes the following parameters:\n",
        "\n",
        "* **image**: The input image to be transformed into a blob.\n",
        "* **scalefactor**: The scale factor used to divide the image pixel values. A value of 1/255 is commonly used.\n",
        "* **size**: The size of the output image blob. YOLOv3 model takes 416x416 size as input.\n",
        "* **mean**: The mean value subtracted from each channel of the image. In this case, (0,0,0) is used as there is no need to subtract any values.\n",
        "* **swapRB**: A boolean flag indicating whether to swap the Red and Blue channels. YOLOv3 requires input images to be in BGR format.\n",
        "* **crop**: A boolean flag indicating whether to crop the image or not. In this case, it is set to False.\n",
        "\n",
        "The **resulting blob** is then used as **input to the YOLOv3 model for object detection**."
      ],
      "metadata": {
        "id": "HlqIcAX_yEXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Image\n",
        "input_img = images[random.randint(0,len(images))]\n",
        "\n",
        "# Convert all images to blobs.\n",
        "blob = image_to_blob(input_img)\n",
        "\n",
        "# Visualize the blobs\n",
        "plt.figure(figsize=(8, 8))\n",
        "\n",
        "# Process the blob.\n",
        "temp_blob = tf.reshape(tf.squeeze(blob), (416,416,3))\n",
        "    \n",
        "# Plot the image\n",
        "plt.imshow(temp_blob)\n",
        "    \n",
        "# Turn off the axis\n",
        "plt.axis('off')\n",
        "\n",
        "# Show the final plot.\n",
        "plt.show()"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2023-03-10T14:42:44.009246Z",
          "iopub.execute_input": "2023-03-10T14:42:44.009643Z",
          "iopub.status.idle": "2023-03-10T14:42:44.607958Z",
          "shell.execute_reply.started": "2023-03-10T14:42:44.009606Z",
          "shell.execute_reply": "2023-03-10T14:42:44.606881Z"
        },
        "trusted": true,
        "id": "6UOPyxHEyEXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set it as input to the network\n",
        "net.setInput(blob)\n",
        "\n",
        "# Apply forward propagation and get the output\n",
        "layerOutputs = net.forward(output_layers_names)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-10T14:42:44.609566Z",
          "iopub.execute_input": "2023-03-10T14:42:44.61021Z",
          "iopub.status.idle": "2023-03-10T14:42:45.800395Z",
          "shell.execute_reply.started": "2023-03-10T14:42:44.610162Z",
          "shell.execute_reply": "2023-03-10T14:42:45.799221Z"
        },
        "trusted": true,
        "id": "HDgkXsnYyEXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have to layer output. Let's first perform a **filter based on the confidence score, or the probability**."
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-09T03:25:11.629424Z",
          "iopub.execute_input": "2023-03-09T03:25:11.630325Z",
          "iopub.status.idle": "2023-03-09T03:25:12.374845Z",
          "shell.execute_reply.started": "2023-03-09T03:25:11.630277Z",
          "shell.execute_reply": "2023-03-09T03:25:12.372733Z"
        },
        "id": "zM5_MFu4yEXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_output_probs(threshold_probability: float, layer_outputs: tuple) -> list:\n",
        "    \n",
        "    \"\"\"\n",
        "    Filters the detections from the output of a YOLO object detection model, by keeping only those with confidence scores\n",
        "    above a specified threshold probability. Returns the bounding boxes, confidence scores, and class IDs of the filtered\n",
        "    detections.\n",
        "\n",
        "    Args:\n",
        "        threshold_probability (float): The minimum confidence score for a detection to be included. Must be in the range [0, 1].\n",
        "        layerOutputs (List): The output layers of a YOLO object detection model.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[List[int], List[float], List[int]]: A tuple containing:\n",
        "        - a list of bounding boxes, where each box is represented by a list of four integers (x_min, y_min, box_width, box_height);\n",
        "        - a list of confidence scores, where each score is a float between 0 and 1;\n",
        "        - a list of class IDs, where each ID is an integer corresponding to the index of the detected class label in the model's class labels list.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Check validity of threshold_probability value\n",
        "    if threshold_probability is None:\n",
        "        raise ValueError(\"Invalid value for threshold_probability. Value cannot be `None`.\")\n",
        "    elif threshold_probability < 0 or threshold_probability > 1.0:\n",
        "        raise ValueError(f\"Cannot assign value {threshold_probability} to threshold_probability, value must be in range [0-1]\")\n",
        "    \n",
        "    # Initialize variables to store object detection results\n",
        "    boxes = []        # Bounding box coordinates for each detected object\n",
        "    confidences = []  # Confidence score for each detected object\n",
        "    class_ids = []    # Class ID for each detected object\n",
        "\n",
        "    # Loop over the output layers from the YOLO model\n",
        "    for output in layer_outputs:\n",
        "\n",
        "        # Loop over each detection in the output\n",
        "        for detection in output:\n",
        "\n",
        "            # Extract the class probabilities and class ID for the current detection\n",
        "            probabilities = detection[5:]\n",
        "            class_id = np.argmax(probabilities)\n",
        "\n",
        "            # Extract the confidence (probability) for the current detection\n",
        "            confidence = probabilities[class_id]\n",
        "\n",
        "            # Filter out weak detections with confidence below the given threshold_probability\n",
        "            if confidence > threshold_probability:\n",
        "\n",
        "                # Extract the bounding box coordinates and scale them to the size of the input image\n",
        "                box = detection[0:4] * np.array([IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_HEIGHT])\n",
        "\n",
        "                # Convert the YOLO-format bounding box (center_x, center_y, width, height) to\n",
        "                # the OpenCV-format bounding box (x_min, y_min, width, height) for drawing later\n",
        "                center_x, center_y, box_width, box_height = box.astype('int')\n",
        "                x_min = int(center_x - (box_width / 2))\n",
        "                y_min = int(center_y - (box_height / 2))\n",
        "\n",
        "                # Add the results to the output lists\n",
        "                boxes.append([x_min, y_min, int(box_width), int(box_height)])\n",
        "                confidences.append(float(confidence))\n",
        "                class_ids.append(class_id)\n",
        "                \n",
        "    # Return the filtered results\n",
        "    return boxes, confidences, class_ids"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-10T14:42:45.804132Z",
          "iopub.execute_input": "2023-03-10T14:42:45.804562Z",
          "iopub.status.idle": "2023-03-10T14:42:45.818552Z",
          "shell.execute_reply.started": "2023-03-10T14:42:45.804522Z",
          "shell.execute_reply": "2023-03-10T14:42:45.816931Z"
        },
        "trusted": true,
        "id": "ctfvaLW4yEXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In **YOLO**, the **bounding box coordinates** are **normalized values** that range from **0 to 1** relative to the **size of the input image**. However, we need to convert these **normalized coordinates** to the **coordinates on the original image** to **draw the bounding box**. Therefore, **we multiply the normalized coordinates** by the **width and height** of the **original image** to get **the actual pixel values.**\n",
        "\n",
        "For example, if the **normalized bounding box coordinates** are **[0.1, 0.2, 0.3, 0.4]** and the **original image size is (800, 600),** then the **actual bounding box** coordinates will be **[80, 120, 240, 240]**. We get these values by **multiplying [0.1, 0.2, 0.3, 0.4] with [800, 600, 800, 600], respectively**."
      ],
      "metadata": {
        "id": "rTSWf7FryEXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out all the predicted bounding boxes based on their probabilities\n",
        "boxes, confidences, class_ids = filter_output_probs(PROB_THRESH, layerOutputs)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-10T14:42:45.820428Z",
          "iopub.execute_input": "2023-03-10T14:42:45.820859Z",
          "iopub.status.idle": "2023-03-10T14:42:45.898145Z",
          "shell.execute_reply.started": "2023-03-10T14:42:45.820799Z",
          "shell.execute_reply": "2023-03-10T14:42:45.896804Z"
        },
        "trusted": true,
        "id": "LeyPgbd7yEXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confidences"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-10T14:42:45.899977Z",
          "iopub.execute_input": "2023-03-10T14:42:45.900421Z",
          "iopub.status.idle": "2023-03-10T14:42:45.9079Z",
          "shell.execute_reply.started": "2023-03-10T14:42:45.900378Z",
          "shell.execute_reply": "2023-03-10T14:42:45.906612Z"
        },
        "trusted": true,
        "id": "2UW6WlM8yEXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After applying the **YOLO model** on the **input images**, it **generates a large number of bounding boxes** for **each image** with their respective **confidence scores**. However, many of **these bounding boxes** may **overlap and represent the same object**. To filter out these **redundant boxes**, we need to **perform non-maximum suppression (NMS)**. **NMS** is a **process that removes overlapping boxes based on their Intersection over Union (IoU) score**.\n",
        "\n",
        "After **applying probability-based filtration** on the **predicted bounding boxes**, we can use **OpenCV's dnn.NMSBoxes()** function to **perform NMS** on the **remaining boxes**. However, this **function only applies NMS to one output at a time**. Therefore, we need to create a **custom function** that can **apply NMS to multiple outputs generated by the YOLO model.**"
      ],
      "metadata": {
        "id": "HlWie2EuyEX1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply non max suppression to all the predictions\n",
        "indicies = cv.dnn.NMSBoxes(boxes, confidences, PROB_THRESH, IoU_THRESH, top_k=5)\n",
        "indicies"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-10T14:42:45.90953Z",
          "iopub.execute_input": "2023-03-10T14:42:45.909986Z",
          "iopub.status.idle": "2023-03-10T14:42:45.928647Z",
          "shell.execute_reply.started": "2023-03-10T14:42:45.909943Z",
          "shell.execute_reply": "2023-03-10T14:42:45.927155Z"
        },
        "trusted": true,
        "id": "HJbK1yUgyEX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_bounding_boxes(input_img: np.ndarray, boxes: list, class_ids: list, confidences: list, labels: list, indicies: list) -> None:\n",
        "    \"\"\"\n",
        "    Draw bounding boxes around the detected objects in an image and label them with their corresponding class names and confidences.\n",
        "    \n",
        "    Args:\n",
        "        input_img (np.ndarray): The input image in the format of numpy array.\n",
        "        boxes (list): A list of boxes for each detected object in the format of [x, y, w, h], where x and y are the coordinates of the top-left corner of the box, and w and h are the width and height of the box.\n",
        "        class_ids (list): A list of class IDs for each detected object.\n",
        "        confidences (list): A list of confidences for each detected object.\n",
        "        labels (list): A list of class names corresponding to their IDs.\n",
        "    \n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    \n",
        "    # Create a copy of the input image\n",
        "    image_temp = input_img.copy()\n",
        "    \n",
        "    \n",
        "    # Loop over each index in indicies\n",
        "    for index in indicies:\n",
        "        \n",
        "        # Get the box coordinates and confidence for the current index\n",
        "        x, y, w, h = boxes[index]\n",
        "        confidence = confidences[index]\n",
        "        \n",
        "        # Set the color randomly\n",
        "        color = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))\n",
        "\n",
        "        \n",
        "        # Draw a bounding box around the object\n",
        "        cv.rectangle(image_temp, (x, y), (x+w, y+h), color=color, thickness=2)\n",
        "        \n",
        "        # Create a text label for the object class and confidence\n",
        "        text_label = '{}:{:.2f}'.format(labels[class_ids[index]], confidence)\n",
        "        \n",
        "        # Set the font face and scale\n",
        "        font_face = cv.FONT_HERSHEY_DUPLEX\n",
        "        font_scale = 1.5\n",
        "        font_thickness = 2\n",
        "        \n",
        "        # Draw the text label above the bounding box\n",
        "        cv.putText(\n",
        "            img=image_temp, \n",
        "            text=text_label, \n",
        "            org=(x+5, y-10),\n",
        "            fontFace=font_face, \n",
        "            fontScale=font_scale,\n",
        "            color=color, # Set the font color to random\n",
        "            thickness=font_thickness\n",
        "        )\n",
        "    \n",
        "    # Display the image\n",
        "    plt.imshow(image_temp)\n",
        "    plt.axis('off')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-10T14:42:45.931774Z",
          "iopub.execute_input": "2023-03-10T14:42:45.932378Z",
          "iopub.status.idle": "2023-03-10T14:42:45.945245Z",
          "shell.execute_reply.started": "2023-03-10T14:42:45.932314Z",
          "shell.execute_reply": "2023-03-10T14:42:45.944042Z"
        },
        "trusted": true,
        "id": "O3l8RqKLyEX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "draw_bounding_boxes(input_img, boxes, class_ids, confidences, labels, indicies)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-10T14:42:45.946454Z",
          "iopub.execute_input": "2023-03-10T14:42:45.946861Z",
          "iopub.status.idle": "2023-03-10T14:42:46.281765Z",
          "shell.execute_reply.started": "2023-03-10T14:42:45.946821Z",
          "shell.execute_reply": "2023-03-10T14:42:46.280077Z"
        },
        "trusted": true,
        "id": "efcltrFfyEX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's add everything into one function."
      ],
      "metadata": {
        "id": "ACvNBVpgyEX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pred_bbs(image: np.ndarray, threshold_probability: float=0.9, iou_threshold: float=0.5, labels: list=labels) -> None:\n",
        "    \"\"\"\n",
        "    Perform object detection on an input image using a pre-trained neural network, and draw bounding boxes around detected objects.\n",
        "    \n",
        "    Args:\n",
        "    - image: A numpy array representing the input image.\n",
        "    - threshold_probability: A float representing the minimum probability threshold for a detected object to be considered valid. Defaults to 0.9.\n",
        "    - iou_threshold: A float representing the intersection over union threshold for Non-Maximum Suppression (NMS). Defaults to 0.5.\n",
        "    - labels: A list of strings representing the class labels for the detected objects. Defaults to the global 'labels' variable.\n",
        "    \n",
        "    Returns:\n",
        "    - None.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Convert image to blob\n",
        "    blob = image_to_blob(image)\n",
        "    \n",
        "    # Make prediction\n",
        "    net.setInput(blob)\n",
        "    output_layers_names = net.getUnconnectedOutLayersNames()\n",
        "    outputs = net.forward(output_layers_names)\n",
        "    \n",
        "    # Filter the inputs based on probability\n",
        "    bboxes, confidences, class_ids = filter_output_probs(threshold_probability = threshold_probability, layer_outputs = outputs)\n",
        "    \n",
        "    # Apply Non Max Suppression\n",
        "    indicies = cv.dnn.NMSBoxes(bboxes = bboxes, scores = confidences, score_threshold = threshold_probability, nms_threshold = iou_threshold)\n",
        "    \n",
        "    # Draw the Bounding Box\n",
        "    draw_bounding_boxes(input_img = image, boxes = bboxes, class_ids = class_ids, confidences = confidences, labels = labels, indicies = indicies)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-10T14:42:46.284028Z",
          "iopub.execute_input": "2023-03-10T14:42:46.285143Z",
          "iopub.status.idle": "2023-03-10T14:42:46.297266Z",
          "shell.execute_reply.started": "2023-03-10T14:42:46.285046Z",
          "shell.execute_reply": "2023-03-10T14:42:46.29571Z"
        },
        "trusted": true,
        "id": "c_AFKY8tyEX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Constant\n",
        "N_IMAGES = 25\n",
        "\n",
        "def plot_random_images(images: np.ndarray) -> None:\n",
        "    \"\"\"\n",
        "    Plots a grid of N_IMAGES random images along with the detected objects bounding boxes.\n",
        "    \n",
        "    Args:\n",
        "    - images (np.ndarray): Array of images to plot.\n",
        "    \n",
        "    Returns:\n",
        "    - None\n",
        "    \"\"\"\n",
        "    \n",
        "    # Plotting Configuration\n",
        "    plt.figure(figsize=(30,20))\n",
        "    \n",
        "    # For n number of images\n",
        "    for n in range(N_IMAGES):\n",
        "        \n",
        "        # Subplot \n",
        "        plt.subplot(int(np.sqrt(N_IMAGES)), int(np.sqrt(N_IMAGES)), n+1)\n",
        "        \n",
        "        # Select Image Randomly\n",
        "        image = images[np.random.randint(len(images))]\n",
        "        \n",
        "        # Detcted objects\n",
        "        pred_bbs(image)\n",
        "        \n",
        "    # Show final plot\n",
        "    plt.show()\n",
        "    \n",
        "# Plot images\n",
        "plot_random_images(images)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2023-03-10T14:42:46.299501Z",
          "iopub.execute_input": "2023-03-10T14:42:46.300162Z",
          "iopub.status.idle": "2023-03-10T14:43:09.122347Z",
          "shell.execute_reply.started": "2023-03-10T14:42:46.300099Z",
          "shell.execute_reply": "2023-03-10T14:43:09.120974Z"
        },
        "trusted": true,
        "id": "bwQFMOFLyEX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Excellent!** The model's **high confidence** in its **predictions** is a **strong indication** of its **robustness and accuracy**. With almost **all predictions being correct**. This is **particularly impressive** given the **complexity and variability of the objects being detected**. It is a testament to the **quality of the training data**, the **effectiveness of the model architecture**, and the **skillful tuning of the hyperparameters**. Overall, this is a **highly promising result for the model's potential real-world applications**.\n",
        "\n",
        "---\n",
        "**DeepNets**"
      ],
      "metadata": {
        "id": "4xgYNBlJyEX8"
      }
    }
  ]
}